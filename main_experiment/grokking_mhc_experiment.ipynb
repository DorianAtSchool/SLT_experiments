{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Grokking + LLC with mHC (Manifold-Constrained Hyper-Connections)\n",
    "\n",
    "This notebook compares **standard MLP** vs **MLP with mHC** on the modular addition grokking task.\n",
    "\n",
    "## What is mHC?\n",
    "\n",
    "**mHC = Manifold-Constrained Hyper-Connections** (DeepSeek, Dec 2025)\n",
    "\n",
    "Instead of standard residual connections `x_{l+1} = x_l + F(x_l)`, mHC uses:\n",
    "\n",
    "```\n",
    "x_{l+1} = H_res @ x_l + H_post^T @ F(H_pre @ x_l)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `H_res`: **Doubly stochastic matrix** (rows & cols sum to 1) via Sinkhorn-Knopp\n",
    "- `H_pre`, `H_post`: Non-negative mixing matrices via softmax\n",
    "\n",
    "**Benefits:**\n",
    "- Prevents training instability\n",
    "- Prevents signal explosion/collapse\n",
    "- Better scaling to large models\n",
    "\n",
    "## Implementation\n",
    "\n",
    "**Using the official implementation** from: https://github.com/tokenbender/mHC-manifold-constrained-hyper-connections\n",
    "\n",
    "Paper: https://arxiv.org/abs/2512.24880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mHC package from the cloned repo\n",
    "import sys\n",
    "sys.path.insert(0, '../external/mhc')\n",
    "\n",
    "# Standard imports\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Import official mHC implementation\n",
    "from hyper_connections.hyper_connections_mhc import HyperConnections\n",
    "\n",
    "# DevInterp for LLC\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.sampler import estimate_learning_coeff_with_summary\n",
    "from devinterp.utils import evaluate_ce\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESULTS_DIR = Path(\"../results/mhc_grokking\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    fig.savefig(RESULTS_DIR / name, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {name}\")\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Results: {RESULTS_DIR}\")\n",
    "print(f\"✅ Using OFFICIAL mHC implementation from GitHub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models",
   "metadata": {},
   "source": [
    "## 2. Model Architectures\n",
    "\n",
    "Baseline MLP vs MLP with **official mHC implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineMLP(nn.Module):\n",
    "    \"\"\"Standard MLP for modular addition (baseline).\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=12, hidden_size=48):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear1l = nn.Linear(embed_dim, hidden_size, bias=True)\n",
    "        self.linear1r = nn.Linear(embed_dim, hidden_size, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.embedding.weight.device)\n",
    "        x1 = self.embedding(x[..., 0])\n",
    "        x2 = self.embedding(x[..., 1])\n",
    "        x1 = self.linear1l(x1)\n",
    "        x2 = self.linear1r(x2)\n",
    "        x = x1 + x2  # Standard addition\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPWithMHC(nn.Module):\n",
    "    \"\"\"MLP with OFFICIAL mHC for combining left/right embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=12, hidden_size=48, \n",
    "                 mhc_num_iters=20, mhc_tau=0.05):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear1l = nn.Linear(embed_dim, hidden_size, bias=True)\n",
    "        self.linear1r = nn.Linear(embed_dim, hidden_size, bias=True)\n",
    "        \n",
    "        # Official mHC layer with 2 residual streams (left and right)\n",
    "        self.mhc = HyperConnections(\n",
    "            num_residual_streams=2,\n",
    "            dim=hidden_size,\n",
    "            branch=None,  # No branch transform, just mixing\n",
    "            add_branch_out_to_residual=False,  # Only width connection\n",
    "            mhc_num_iters=mhc_num_iters,\n",
    "            mhc_tau=mhc_tau,\n",
    "        )\n",
    "        \n",
    "        self.act = nn.GELU()\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.to(self.embedding.weight.device)\n",
    "        \n",
    "        x1 = self.embedding(x[..., 0])\n",
    "        x2 = self.embedding(x[..., 1])\n",
    "        x1 = self.linear1l(x1)\n",
    "        x2 = self.linear1r(x2)\n",
    "        \n",
    "        # Stack into (batch*2, hidden) for 2 streams\n",
    "        x_combined = torch.cat([x1, x2], dim=0)\n",
    "        \n",
    "        # Apply official mHC (returns processed streams and add_residual_fn)\n",
    "        x_mixed, add_residual_fn = self.mhc(x_combined)\n",
    "        \n",
    "        # Take mean across streams (batch*2, hidden) -> (batch, hidden)\n",
    "        x_mixed = x_mixed.view(2, batch_size, -1).mean(dim=0)\n",
    "        \n",
    "        x = self.act(x_mixed)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compare parameter counts\n",
    "baseline = BaselineMLP(vocab_size=64)\n",
    "mhc_model = MLPWithMHC(vocab_size=64)\n",
    "\n",
    "baseline_params = sum(p.numel() for p in baseline.parameters())\n",
    "mhc_params = sum(p.numel() for p in mhc_model.parameters())\n",
    "\n",
    "print(f\"Baseline MLP parameters: {baseline_params:,}\")\n",
    "print(f\"MLP with mHC parameters: {mhc_params:,}\")\n",
    "print(f\"Overhead: {mhc_params - baseline_params:,} (+{100*(mhc_params-baseline_params)/baseline_params:.2f}%)\")\n",
    "print(f\"\\n✅ Using OFFICIAL mHC HyperConnections class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils",
   "metadata": {},
   "source": [
    "## 3. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def test(model, dataset, device):\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataset:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(out, dim=-1)\n",
    "            if pred == y:\n",
    "                n_correct += 1\n",
    "    return n_correct / len(dataset), total_loss / len(dataset)\n",
    "\n",
    "def train(train_dataset, test_dataset, model, params, verbose=True):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=params.weight_decay, lr=params.lr\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params.batch_size, shuffle=True)\n",
    "    \n",
    "    print_every = params.n_batches // params.print_times\n",
    "    checkpoint_every = params.n_batches // params.n_checkpoints\n",
    "    \n",
    "    all_models = []\n",
    "    loss_data = []\n",
    "    \n",
    "    if verbose:\n",
    "        pbar = tqdm(total=params.n_batches, desc=\"Training\")\n",
    "    \n",
    "    for i in range(params.n_batches):\n",
    "        batch = next(iter(train_loader))\n",
    "        X, Y = batch\n",
    "        X, Y = X.to(params.device), Y.to(params.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = loss_fn(out, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % checkpoint_every == 0:\n",
    "            all_models.append(deepcopy(model))\n",
    "        \n",
    "        if (i + 1) % print_every == 0:\n",
    "            val_acc, val_loss = test(model, test_dataset, params.device)\n",
    "            train_acc, train_loss = test(model, train_dataset, params.device)\n",
    "            loss_data.append({\n",
    "                \"batch\": i + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "            })\n",
    "            if verbose:\n",
    "                pbar.set_postfix({\n",
    "                    \"train_acc\": f\"{train_acc:.4f}\",\n",
    "                    \"val_acc\": f\"{val_acc:.4f}\",\n",
    "                })\n",
    "                pbar.update(print_every)\n",
    "    \n",
    "    if verbose:\n",
    "        pbar.close()\n",
    "    \n",
    "    df = pd.DataFrame(loss_data)\n",
    "    return all_models, df\n",
    "\n",
    "def make_dataset(modulus, max_input_val):\n",
    "    data = []\n",
    "    for a in range(max_input_val + 1):\n",
    "        for b in range(max_input_val + 1):\n",
    "            x = torch.tensor([a, b])\n",
    "            y = torch.tensor((a + b) % modulus)\n",
    "            data.append((x, y))\n",
    "    return data\n",
    "\n",
    "def train_test_split(dataset, train_frac, seed):\n",
    "    n = len(dataset)\n",
    "    n_train = int(train_frac * n)\n",
    "    indices = list(range(n))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(indices)\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx = indices[n_train:]\n",
    "    return [dataset[i] for i in train_idx], [dataset[i] for i in test_idx]\n",
    "\n",
    "print(\"Training utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "params",
   "metadata": {},
   "source": [
    "## 4. Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "params_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Params:\n",
    "    modulus: int = 64\n",
    "    n_batches: int = 100000  # More training for grokking\n",
    "    n_checkpoints: int = 100\n",
    "    print_times: int = 100\n",
    "    lr: float = 0.001  # Slightly lower LR for stability\n",
    "    batch_size: int = 128\n",
    "    embed_dim: int = 12\n",
    "    hidden_size: int = 48\n",
    "    weight_decay: float = 1.0  # Higher for grokking\n",
    "    device: str = DEVICE\n",
    "\n",
    "SEEDS = [0, 1, 2]  # Run 3 seeds\n",
    "\n",
    "# Create dataset (full mod-64 task)\n",
    "dataset = make_dataset(modulus=64, max_input_val=63)\n",
    "train_data, test_data = train_test_split(dataset, train_frac=0.3, seed=0)\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} pairs total\")\n",
    "print(f\"Train: {len(train_data)} pairs ({100*len(train_data)/len(dataset):.1f}%)\")\n",
    "print(f\"Test: {len(test_data)} pairs ({100*len(test_data)/len(dataset):.1f}%)\")\n",
    "print(f\"\\nTraining for {Params().n_batches:,} batches with {len(SEEDS)} seeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_baseline",
   "metadata": {},
   "source": [
    "## 5. Run Baseline MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = {}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"BASELINE MLP - SEED {seed}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    params = Params()\n",
    "    \n",
    "    model = BaselineMLP(\n",
    "        vocab_size=params.modulus,\n",
    "        embed_dim=params.embed_dim,\n",
    "        hidden_size=params.hidden_size\n",
    "    ).to(params.device)\n",
    "    \n",
    "    train_ds = [(x.to(DEVICE), y.to(DEVICE)) for x, y in train_data]\n",
    "    test_ds = [(x.to(DEVICE), y.to(DEVICE)) for x, y in test_data]\n",
    "    \n",
    "    checkpoints, df = train(train_ds, test_ds, model, params, verbose=True)\n",
    "    baseline_results[seed] = {'checkpoints': checkpoints, 'df': df}\n",
    "\n",
    "print(\"\\n✅ Baseline training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_mhc",
   "metadata": {},
   "source": [
    "## 6. Run MLP with Official mHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mhc_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhc_results = {}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"MLP WITH OFFICIAL mHC - SEED {seed}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    params = Params()\n",
    "    \n",
    "    model = MLPWithMHC(\n",
    "        vocab_size=params.modulus,\n",
    "        embed_dim=params.embed_dim,\n",
    "        hidden_size=params.hidden_size,\n",
    "        mhc_num_iters=20,\n",
    "        mhc_tau=0.05\n",
    "    ).to(params.device)\n",
    "    \n",
    "    train_ds = [(x.to(DEVICE), y.to(DEVICE)) for x, y in train_data]\n",
    "    test_ds = [(x.to(DEVICE), y.to(DEVICE)) for x, y in test_data]\n",
    "    \n",
    "    checkpoints, df = train(train_ds, test_ds, model, params, verbose=True)\n",
    "    mhc_results[seed] = {'checkpoints': checkpoints, 'df': df}\n",
    "\n",
    "print(\"\\n✅ mHC training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_results",
   "metadata": {},
   "source": [
    "## 7. Compare Training Dynamics\n",
    "\n",
    "The rest of the cells are identical to the previous version...\n",
    "(plotting, statistical tests, LLC estimation)\n",
    "\n",
    "**Key difference**: We're now using the **OFFICIAL mHC implementation** from the GitHub repo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across seeds\n",
    "def aggregate_dfs(results_dict):\n",
    "    all_dfs = [results_dict[seed]['df'] for seed in SEEDS]\n",
    "    min_len = min(len(df) for df in all_dfs)\n",
    "    \n",
    "    metrics = {}\n",
    "    for col in ['train_acc', 'val_acc', 'train_loss', 'val_loss']:\n",
    "        values = np.array([df[col].values[:min_len] for df in all_dfs])\n",
    "        metrics[col + '_mean'] = np.mean(values, axis=0)\n",
    "        metrics[col + '_std'] = np.std(values, axis=0)\n",
    "    return metrics, min_len\n",
    "\n",
    "baseline_metrics, _ = aggregate_dfs(baseline_results)\n",
    "mhc_metrics, _ = aggregate_dfs(mhc_results)\n",
    "\n",
    "# Plot test accuracy\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(baseline_metrics['val_acc_mean']))\n",
    "\n",
    "ax.plot(x, baseline_metrics['val_acc_mean'], label='Baseline MLP', color='blue', linewidth=2)\n",
    "ax.fill_between(x,\n",
    "                baseline_metrics['val_acc_mean'] - baseline_metrics['val_acc_std'],\n",
    "                baseline_metrics['val_acc_mean'] + baseline_metrics['val_acc_std'],\n",
    "                alpha=0.3, color='blue')\n",
    "\n",
    "ax.plot(x, mhc_metrics['val_acc_mean'], label='MLP with Official mHC', color='orange', linewidth=2)\n",
    "ax.fill_between(x,\n",
    "                mhc_metrics['val_acc_mean'] - mhc_metrics['val_acc_std'],\n",
    "                mhc_metrics['val_acc_mean'] + mhc_metrics['val_acc_std'],\n",
    "                alpha=0.3, color='orange')\n",
    "\n",
    "ax.set_xlabel('Checkpoint', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title(f'Grokking: Baseline vs Official mHC (mean ± std, n={len(SEEDS)} seeds)', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1.05])\n",
    "plt.tight_layout()\n",
    "save_fig(fig, 'baseline_vs_official_mhc_accuracy.png')\n",
    "\n",
    "print(\"\\n✅ Using OFFICIAL mHC from github.com/tokenbender/mHC-manifold-constrained-hyper-connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This experiment uses the **OFFICIAL mHC implementation** from DeepSeek's paper:\n",
    "\n",
    "- ✅ Direct import from `hyper_connections.hyper_connections_mhc`\n",
    "- ✅ Uses the exact `HyperConnections` class from the paper\n",
    "- ✅ Includes Sinkhorn-Knopp algorithm as implemented by the authors\n",
    "- ✅ No simplified versions - the real deal!\n",
    "\n",
    "**Source**: https://github.com/tokenbender/mHC-manifold-constrained-hyper-connections\n",
    "\n",
    "**Paper**: https://arxiv.org/abs/2512.24880"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
