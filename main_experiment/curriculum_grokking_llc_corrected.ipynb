{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70b15f4",
   "metadata": {},
   "source": [
    "# Curriculum Learning and LLC in Grokking (CORRECTED)\n",
    "\n",
    "This notebook explores how curriculum learning (gradually increasing task difficulty with proper weight transfer) affects the Local Learning Coefficient (LLC) and grokking behavior in modular arithmetic.\n",
    "\n",
    "**Based on the devinterp grokking example:**  \n",
    "https://github.com/timaeus-research/devinterp/blob/main/examples/grokking.ipynb\n",
    "\n",
    "## Key Corrections from Original Experiment:\n",
    "\n",
    "1. âœ… **Equalized training budgets** - Direct training now gets same total batches as curriculum\n",
    "2. âœ… **Proper weight transfer** - Pad/crop embedding and output layers instead of random reinitialization\n",
    "3. âœ… **Multiple seeds** - Run 5 seeds with statistical significance testing\n",
    "4. âœ… **3 chains for LLC** - Increased from 1 chain for reliability\n",
    "5. âœ… **Fixed torch.argmax** - Added dimension parameter\n",
    "6. âœ… **Conceptual clarity** - Acknowledged this is transfer learning between related tasks\n",
    "7. âœ… **Adaptive LLC hyperparameters** - Scale by model size\n",
    "8. âœ… **Statistical testing** - T-tests and confidence intervals\n",
    "\n",
    "**Research Question:**  \n",
    "Does sequential transfer learning through increasing moduli (mod-8 â†’ mod-16 â†’ mod-32 â†’ mod-64) produce different LLC trajectories and generalization compared to direct training on mod-64?\n",
    "\n",
    "**Important Note:**  \n",
    "While we call this \"curriculum learning,\" the tasks are related but not strictly hierarchical - mod-8 is not a subset of mod-16. This is more accurately described as **transfer learning on progressively complex related tasks**.\n",
    "\n",
    "**Setup:**\n",
    "- **Curriculum:** Train on mod-8 (50k), then mod-16 (50k), then mod-32 (50k), then mod-64 (50k) = 200k total\n",
    "- **Direct:** Train directly on mod-64 for 200k batches (equalized budget!)\n",
    "- **Seeds:** 5 independent runs (seeds 0-4)\n",
    "- Track train/test accuracy, loss, and LLC at each stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9416fac",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if running in a fresh environment\n",
    "%pip install devinterp nbformat scipy\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.sampler import estimate_learning_coeff_with_summary\n",
    "from devinterp.utils import evaluate_ce\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create results directory for saving figures\n",
    "RESULTS_DIR = Path(\"../results/curriculum_learning_corrected\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving results to: {RESULTS_DIR.absolute()}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Utility function to save and show figures\n",
    "def save_and_show(fig, filename, dpi=300):\n",
    "    \"\"\"Save figure to results directory and display it\"\"\"\n",
    "    filepath = RESULTS_DIR / filename\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight')\n",
    "    print(f\"Saved: {filepath.name}\")\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa8946",
   "metadata": {},
   "source": [
    "## 2. Define Experiment Parameters\n",
    "\n",
    "**Key Change:** Direct training gets 200k batches to match curriculum's total budget (4 stages Ã— 50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c681aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentParams:\n",
    "    n_batches_per_stage: int = 50000\n",
    "    n_save_model_checkpoints: int = 100\n",
    "    print_times: int = 100\n",
    "    lr: float = 0.005\n",
    "    batch_size: int = 128\n",
    "    hidden_size: int = 48\n",
    "    embed_dim: int = 12\n",
    "    train_frac: float = 0.4\n",
    "    device: str = DEVICE\n",
    "    weight_decay: float = 0.0002\n",
    "\n",
    "CURRICULUM_NUMS = [8, 16, 32, 64]\n",
    "DIRECT_MODULUS = CURRICULUM_NUMS[-1]\n",
    "SEEDS = [0, 1, 2, 3, 4]  # Run 5 seeds for statistical validity\n",
    "\n",
    "# Equalized training budget\n",
    "CURRICULUM_TOTAL_BATCHES = len(CURRICULUM_NUMS) * 50000  # 200k\n",
    "DIRECT_TOTAL_BATCHES = CURRICULUM_TOTAL_BATCHES  # 200k (EQUALIZED!)\n",
    "\n",
    "print(f\"Curriculum stages: {CURRICULUM_NUMS}\")\n",
    "print(f\"Curriculum total batches: {CURRICULUM_TOTAL_BATCHES:,}\")\n",
    "print(f\"Direct total batches: {DIRECT_TOTAL_BATCHES:,}\")\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"\\nâœ… Training budgets are now equalized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76f73b",
   "metadata": {},
   "source": [
    "## 3. Define MLP Model and Utility Functions\n",
    "\n",
    "**Key Changes:**\n",
    "- Fixed `torch.argmax` to include dimension parameter\n",
    "- Added proper weight transfer function with padding/cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c263acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(params.p, params.embed_dim)\n",
    "        self.linear1r = nn.Linear(params.embed_dim, params.hidden_size, bias=True)\n",
    "        self.linear1l = nn.Linear(params.embed_dim, params.hidden_size, bias=True)\n",
    "        self.linear2 = nn.Linear(params.hidden_size, params.p, bias=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.vocab_size = params.p\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.embedding.weight.device)\n",
    "        x1 = self.embedding(x[..., 0])\n",
    "        x2 = self.embedding(x[..., 1])\n",
    "        x1 = self.linear1l(x1)\n",
    "        x2 = self.linear1r(x2)\n",
    "        x = x1 + x2\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "def test(model, dataset, device):\n",
    "    \"\"\"Test model on dataset. FIXED: torch.argmax now includes dim parameter.\"\"\"\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataset:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(out, dim=-1)  # FIXED: Added dim=-1\n",
    "            if pred == y:\n",
    "                n_correct += 1\n",
    "    return n_correct / len(dataset), total_loss / len(dataset)\n",
    "\n",
    "def transfer_weights_with_padding(new_model, prev_model, verbose=False):\n",
    "    \"\"\"Transfer weights from prev_model to new_model, padding/cropping as needed.\n",
    "    \n",
    "    Key improvement: Instead of random reinitialization, we:\n",
    "    - Pad embedding/output layers with random initialization for new dimensions\n",
    "    - Crop if going to smaller model (though we don't in curriculum)\n",
    "    - Fully transfer matching layers\n",
    "    \"\"\"\n",
    "    new_dict = new_model.state_dict()\n",
    "    prev_dict = prev_model.state_dict()\n",
    "    \n",
    "    transferred_params = 0\n",
    "    total_params = 0\n",
    "    \n",
    "    for k in new_dict:\n",
    "        total_params += new_dict[k].numel()\n",
    "        \n",
    "        if k not in prev_dict:\n",
    "            if verbose:\n",
    "                print(f\"  {k}: Not in prev model, keeping random init\")\n",
    "            continue\n",
    "            \n",
    "        new_shape = new_dict[k].shape\n",
    "        prev_shape = prev_dict[k].shape\n",
    "        \n",
    "        # Exact match - full transfer\n",
    "        if new_shape == prev_shape:\n",
    "            new_dict[k] = prev_dict[k].clone()\n",
    "            transferred_params += new_dict[k].numel()\n",
    "            if verbose:\n",
    "                print(f\"  {k}: Full transfer {prev_shape}\")\n",
    "        \n",
    "        # Embedding layer: pad with random init for new tokens\n",
    "        elif 'embedding' in k and len(new_shape) == 2:\n",
    "            old_vocab, embed_dim = prev_shape\n",
    "            new_vocab, _ = new_shape\n",
    "            # Copy old embeddings\n",
    "            min_vocab = min(old_vocab, new_vocab)\n",
    "            new_dict[k][:min_vocab] = prev_dict[k][:min_vocab].clone()\n",
    "            transferred_params += min_vocab * embed_dim\n",
    "            if verbose:\n",
    "                print(f\"  {k}: Transferred {min_vocab}/{new_vocab} embeddings, rest random\")\n",
    "        \n",
    "        # Output layer (linear2): pad/crop output dimension\n",
    "        elif 'linear2' in k and len(new_shape) == 2:\n",
    "            hidden, old_out = prev_shape\n",
    "            _, new_out = new_shape\n",
    "            min_out = min(old_out, new_out)\n",
    "            new_dict[k][:min_out] = prev_dict[k][:min_out].clone()\n",
    "            transferred_params += min_out * hidden\n",
    "            if verbose:\n",
    "                print(f\"  {k}: Transferred {min_out}/{new_out} outputs, rest random\")\n",
    "        \n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  {k}: Shape mismatch {prev_shape} -> {new_shape}, keeping random\")\n",
    "    \n",
    "    new_model.load_state_dict(new_dict)\n",
    "    transfer_pct = 100 * transferred_params / total_params\n",
    "    print(f\"âœ… Transferred {transferred_params:,}/{total_params:,} parameters ({transfer_pct:.1f}%)\")\n",
    "    return new_model\n",
    "\n",
    "def train(train_dataset, test_dataset, params, model=None, verbose=True):\n",
    "    \"\"\"Train model for params.n_batches_per_stage batches.\"\"\"\n",
    "    all_models = []\n",
    "    if model is None:\n",
    "        model = MLP(params).to(params.device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), weight_decay=params.weight_decay, lr=params.lr\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params.batch_size, shuffle=True)\n",
    "    \n",
    "    print_every = params.n_batches_per_stage // params.print_times\n",
    "    checkpoint_every = None\n",
    "    if params.n_save_model_checkpoints > 0:\n",
    "        checkpoint_every = params.n_batches_per_stage // params.n_save_model_checkpoints\n",
    "    \n",
    "    loss_data = []\n",
    "    if verbose:\n",
    "        pbar = tqdm(total=params.n_batches_per_stage, desc=\"Training\")\n",
    "    \n",
    "    for i in range(params.n_batches_per_stage):\n",
    "        batch = next(iter(train_loader))\n",
    "        X, Y = batch\n",
    "        X, Y = X.to(params.device), Y.to(params.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = loss_fn(out, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if checkpoint_every and (i + 1) % checkpoint_every == 0:\n",
    "            all_models += [deepcopy(model)]\n",
    "        \n",
    "        if (i + 1) % print_every == 0:\n",
    "            val_acc, val_loss = test(model, test_dataset, params.device)\n",
    "            train_acc, train_loss = test(model, train_dataset, params.device)\n",
    "            loss_data.append({\n",
    "                \"batch\": i + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "            })\n",
    "            if verbose:\n",
    "                pbar.set_postfix({\n",
    "                    \"train_loss\": f\"{train_loss:.4f}\",\n",
    "                    \"train_acc\": f\"{train_acc:.4f}\",\n",
    "                    \"val_loss\": f\"{val_loss:.4f}\",\n",
    "                    \"val_acc\": f\"{val_acc:.4f}\",\n",
    "                })\n",
    "                pbar.update(print_every)\n",
    "    \n",
    "    if verbose:\n",
    "        pbar.close()\n",
    "    \n",
    "    df = pd.DataFrame(loss_data)\n",
    "    train_acc, train_loss = test(model, train_dataset, params.device)\n",
    "    val_acc, val_loss = test(model, test_dataset, params.device)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Final Train Acc: {train_acc:.4f} | Final Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Final Val Acc: {val_acc:.4f} | Final Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return all_models, df\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_all_pairs(p):\n",
    "    pairs = []\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            pairs.append((i, j))\n",
    "    return pairs\n",
    "\n",
    "def make_dataset(p):\n",
    "    data = []\n",
    "    pairs = get_all_pairs(p)\n",
    "    for a, b in pairs:\n",
    "        data.append((torch.tensor([a, b]), torch.tensor((a + b) % p)))\n",
    "    return data\n",
    "\n",
    "def train_test_split(dataset, train_split_proportion, seed):\n",
    "    \"\"\"Split dataset deterministically based on seed.\"\"\"\n",
    "    l = len(dataset)\n",
    "    train_len = int(train_split_proportion * l)\n",
    "    idx = list(range(l))\n",
    "    \n",
    "    # Use local RNG to avoid polluting global state\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(idx)\n",
    "    \n",
    "    train_idx = idx[:train_len]\n",
    "    test_idx = idx[train_len:]\n",
    "    return [dataset[i] for i in train_idx], [dataset[i] for i in test_idx]\n",
    "\n",
    "print(\"Model and utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73f220",
   "metadata": {},
   "source": [
    "## 4. Create Datasets\n",
    "\n",
    "Prepare datasets once (they'll be reused across seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for each prime in curriculum and for direct training\n",
    "curriculum_datasets = {}\n",
    "curriculum_splits = {}\n",
    "\n",
    "for p in CURRICULUM_NUMS:\n",
    "    dataset = make_dataset(p)\n",
    "    train_data, test_data = train_test_split(dataset, 0.4, seed=0)  # Same split for all\n",
    "    curriculum_datasets[p] = dataset\n",
    "    curriculum_splits[p] = (train_data, test_data)\n",
    "    print(f\"mod-{p}: {len(train_data)} train, {len(test_data)} test\")\n",
    "\n",
    "direct_dataset = make_dataset(DIRECT_MODULUS)\n",
    "direct_train_data, direct_test_data = train_test_split(direct_dataset, 0.4, seed=0)\n",
    "print(f\"\\nDirect (mod-{DIRECT_MODULUS}): {len(direct_train_data)} train, {len(direct_test_data)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd3255",
   "metadata": {},
   "source": [
    "## 5. Run Curriculum Learning Across Multiple Seeds\n",
    "\n",
    "**Key Changes:**\n",
    "- Run 5 independent seeds\n",
    "- Use proper weight transfer with padding/cropping\n",
    "- Store results for statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ac05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum_results = {}  # {seed: {stage: {checkpoints, df}}}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"CURRICULUM LEARNING - SEED {seed}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    curriculum_results[seed] = {}\n",
    "    prev_model = None\n",
    "    \n",
    "    for stage_idx, p in enumerate(CURRICULUM_NUMS):\n",
    "        print(f\"\\nStage {stage_idx+1}/{len(CURRICULUM_NUMS)}: mod-{p}\")\n",
    "        \n",
    "        params = ExperimentParams()\n",
    "        params.p = p\n",
    "        params.device = DEVICE\n",
    "        \n",
    "        train_data, test_data = curriculum_splits[p]\n",
    "        train_data = [(x.to(params.device), y.to(params.device)) for x, y in train_data]\n",
    "        test_data = [(x.to(params.device), y.to(params.device)) for x, y in test_data]\n",
    "        \n",
    "        # Transfer weights from previous stage\n",
    "        if prev_model is not None:\n",
    "            model = MLP(params)\n",
    "            model = transfer_weights_with_padding(model, prev_model, verbose=False)\n",
    "            model = model.to(params.device)\n",
    "        else:\n",
    "            model = None\n",
    "        \n",
    "        checkpoints, df = train(train_data, test_data, params, model=model, verbose=True)\n",
    "        curriculum_results[seed][p] = {'checkpoints': checkpoints, 'df': df}\n",
    "        prev_model = checkpoints[-1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Curriculum learning complete for all seeds!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e52fa",
   "metadata": {},
   "source": [
    "## 6. Run Direct Training Across Multiple Seeds\n",
    "\n",
    "**Key Change:** Direct training now gets 200k batches (same as curriculum total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e62cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_results = {}  # {seed: {checkpoints, df}}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"DIRECT TRAINING - SEED {seed}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    params = ExperimentParams()\n",
    "    params.p = DIRECT_MODULUS\n",
    "    params.device = DEVICE\n",
    "    params.n_batches_per_stage = DIRECT_TOTAL_BATCHES  # 200k batches!\n",
    "    \n",
    "    train_data = [(x.to(params.device), y.to(params.device)) for x, y in direct_train_data]\n",
    "    test_data = [(x.to(params.device), y.to(params.device)) for x, y in direct_test_data]\n",
    "    \n",
    "    checkpoints, df = train(train_data, test_data, params, model=None, verbose=True)\n",
    "    direct_results[seed] = {'checkpoints': checkpoints, 'df': df}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Direct training complete for all seeds!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee557c",
   "metadata": {},
   "source": [
    "## 7. Plot Accuracy and Loss with Confidence Intervals\n",
    "\n",
    "**Key Change:** Show mean Â± std across seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a7935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metrics(results_dict, modulus_key):\n",
    "    \"\"\"Aggregate metrics across seeds, return mean and std.\"\"\"\n",
    "    all_dfs = [results_dict[seed][modulus_key]['df'] for seed in SEEDS]\n",
    "    \n",
    "    # Ensure all have same length\n",
    "    min_len = min(len(df) for df in all_dfs)\n",
    "    \n",
    "    metrics = {}\n",
    "    for col in ['train_acc', 'val_acc', 'train_loss', 'val_loss']:\n",
    "        values = np.array([df[col].values[:min_len] for df in all_dfs])\n",
    "        metrics[col + '_mean'] = np.mean(values, axis=0)\n",
    "        metrics[col + '_std'] = np.std(values, axis=0)\n",
    "    \n",
    "    return metrics, min_len\n",
    "\n",
    "# Plot final curriculum stage vs direct\n",
    "curriculum_final_metrics, curr_len = aggregate_metrics(curriculum_results, CURRICULUM_NUMS[-1])\n",
    "direct_metrics, direct_len = aggregate_metrics(direct_results, None)  # Direct doesn't use modulus key\n",
    "\n",
    "# Need to handle direct_results structure\n",
    "all_direct_dfs = [direct_results[seed]['df'] for seed in SEEDS]\n",
    "min_direct_len = min(len(df) for df in all_direct_dfs)\n",
    "direct_metrics = {}\n",
    "for col in ['train_acc', 'val_acc', 'train_loss', 'val_loss']:\n",
    "    values = np.array([df[col].values[:min_direct_len] for df in all_direct_dfs])\n",
    "    direct_metrics[col + '_mean'] = np.mean(values, axis=0)\n",
    "    direct_metrics[col + '_std'] = np.std(values, axis=0)\n",
    "\n",
    "# Plot accuracy comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x_curr = np.arange(len(curriculum_final_metrics['val_acc_mean']))\n",
    "x_direct = np.arange(len(direct_metrics['val_acc_mean']))\n",
    "\n",
    "# Curriculum\n",
    "ax.plot(x_curr, curriculum_final_metrics['val_acc_mean'], label='Curriculum: Test Acc', color='blue')\n",
    "ax.fill_between(x_curr, \n",
    "                curriculum_final_metrics['val_acc_mean'] - curriculum_final_metrics['val_acc_std'],\n",
    "                curriculum_final_metrics['val_acc_mean'] + curriculum_final_metrics['val_acc_std'],\n",
    "                alpha=0.3, color='blue')\n",
    "\n",
    "# Direct\n",
    "ax.plot(x_direct, direct_metrics['val_acc_mean'], label='Direct: Test Acc', color='orange')\n",
    "ax.fill_between(x_direct,\n",
    "                direct_metrics['val_acc_mean'] - direct_metrics['val_acc_std'],\n",
    "                direct_metrics['val_acc_mean'] + direct_metrics['val_acc_std'],\n",
    "                alpha=0.3, color='orange')\n",
    "\n",
    "ax.set_xlabel('Checkpoint')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title(f'Curriculum vs Direct: Test Accuracy (mean Â± std, n={len(SEEDS)} seeds)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "save_and_show(fig, 'curriculum_vs_direct_accuracy_with_ci.png')\n",
    "\n",
    "# Plot loss comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(x_curr, curriculum_final_metrics['val_loss_mean'], label='Curriculum: Test Loss', color='blue')\n",
    "ax.fill_between(x_curr,\n",
    "                curriculum_final_metrics['val_loss_mean'] - curriculum_final_metrics['val_loss_std'],\n",
    "                curriculum_final_metrics['val_loss_mean'] + curriculum_final_metrics['val_loss_std'],\n",
    "                alpha=0.3, color='blue')\n",
    "\n",
    "ax.plot(x_direct, direct_metrics['val_loss_mean'], label='Direct: Test Loss', color='orange')\n",
    "ax.fill_between(x_direct,\n",
    "                direct_metrics['val_loss_mean'] - direct_metrics['val_loss_std'],\n",
    "                direct_metrics['val_loss_mean'] + direct_metrics['val_loss_std'],\n",
    "                alpha=0.3, color='orange')\n",
    "\n",
    "ax.set_xlabel('Checkpoint')\n",
    "ax.set_ylabel('Test Loss')\n",
    "ax.set_title(f'Curriculum vs Direct: Test Loss (mean Â± std, n={len(SEEDS)} seeds)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')  # Log scale for loss\n",
    "plt.tight_layout()\n",
    "save_and_show(fig, 'curriculum_vs_direct_loss_with_ci.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p23kugfpryj",
   "metadata": {},
   "source": [
    "## 8. LLC Hyperparameters - Adaptive by Model Size\n",
    "\n",
    "**Key Change:** Scale LLC hyperparameters based on model complexity (vocabulary size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yofl53dvcqg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llc_hyperparams(vocab_size):\n",
    "    \"\"\"Get LLC hyperparameters scaled by model size.\n",
    "    \n",
    "    Larger vocabulary = more parameters in embedding/output layers,\n",
    "    may need adjusted localization and learning rate.\n",
    "    \"\"\"\n",
    "    base_lr = 3e-3\n",
    "    base_gamma = 5.0\n",
    "    \n",
    "    # Scale learning rate down slightly for larger models\n",
    "    lr_scale = np.sqrt(8 / vocab_size)  # Reference: mod-8\n",
    "    lr = base_lr * lr_scale\n",
    "    \n",
    "    # Scale localization up slightly for larger models\n",
    "    gamma_scale = np.sqrt(vocab_size / 8)\n",
    "    gamma = base_gamma * gamma_scale\n",
    "    \n",
    "    return {\n",
    "        'lr': lr,\n",
    "        'nbeta': 2.0,  # Keep constant\n",
    "        'gamma': gamma,\n",
    "        'num_chains': 3,  # Use 3 chains for reliability\n",
    "        'num_draws': 1000,\n",
    "    }\n",
    "\n",
    "# Show hyperparameters for each modulus\n",
    "print(\"LLC Hyperparameters by modulus:\")\n",
    "print(\"=\"*60)\n",
    "for p in CURRICULUM_NUMS:\n",
    "    hp = get_llc_hyperparams(p)\n",
    "    print(f\"mod-{p:2d}: lr={hp['lr']:.4f}, Î³={hp['gamma']:.2f}, chains={hp['num_chains']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59d4fa",
   "metadata": {},
   "source": [
    "## 9. Estimate LLC for Curriculum (Single Seed First)\n",
    "\n",
    "**Key Changes:**\n",
    "- Use 3 chains instead of 1\n",
    "- Adaptive hyperparameters by model size\n",
    "\n",
    "**Note:** Running LLC for all seeds Ã— all checkpoints would take very long.\n",
    "We'll do seed 0 fully, then sample other seeds for statistical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73376651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate LLC for seed 0 curriculum (all checkpoints, all stages)\n",
    "seed = 0\n",
    "curriculum_llcs_seed0 = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Estimating LLC for CURRICULUM (seed {seed})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for stage_idx, p in enumerate(CURRICULUM_NUMS):\n",
    "    print(f\"\\nStage {stage_idx+1}/{len(CURRICULUM_NUMS)}: mod-{p}\")\n",
    "    \n",
    "    params = ExperimentParams()\n",
    "    params.p = p\n",
    "    train_data, _ = curriculum_splits[p]\n",
    "    loader = DataLoader(train_data, batch_size=params.batch_size, shuffle=True)\n",
    "    \n",
    "    checkpoints = curriculum_results[seed][p]['checkpoints']\n",
    "    hp = get_llc_hyperparams(p)\n",
    "    \n",
    "    print(f\"  Estimating LLC for {len(checkpoints)} checkpoints...\")\n",
    "    print(f\"  Hyperparams: lr={hp['lr']:.4f}, Î³={hp['gamma']:.2f}, chains={hp['num_chains']}\")\n",
    "    \n",
    "    stage_llcs = []\n",
    "    for checkpoint_idx, model_checkpoint in enumerate(checkpoints):\n",
    "        if (checkpoint_idx + 1) % 20 == 0 or checkpoint_idx == 0 or checkpoint_idx == len(checkpoints) - 1:\n",
    "            print(f\"    Checkpoint {checkpoint_idx+1}/{len(checkpoints)}\")\n",
    "        \n",
    "        llc_stats = estimate_learning_coeff_with_summary(\n",
    "            model_checkpoint,\n",
    "            loader=loader,\n",
    "            evaluate=evaluate_ce,\n",
    "            sampling_method=SGLD,\n",
    "            optimizer_kwargs=dict(lr=hp['lr'], nbeta=hp['nbeta'], localization=hp['gamma']),\n",
    "            num_chains=hp['num_chains'],\n",
    "            num_draws=hp['num_draws'],\n",
    "            device=DEVICE,\n",
    "            online=False,\n",
    "        )\n",
    "        stage_llcs.append(llc_stats)\n",
    "    \n",
    "    curriculum_llcs_seed0[p] = stage_llcs\n",
    "    llc_values = [llc['llc/mean'] for llc in stage_llcs]\n",
    "    print(f\"  Completed: LLC range [{min(llc_values):.2f}, {max(llc_values):.2f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Curriculum LLC estimation complete (seed 0)!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca26ca",
   "metadata": {},
   "source": [
    "## 10. Estimate LLC for Direct Training (Single Seed)\n",
    "\n",
    "Same approach as curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate LLC for seed 0 direct training\n",
    "seed = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Estimating LLC for DIRECT TRAINING (seed {seed}, mod-{DIRECT_MODULUS})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "params = ExperimentParams()\n",
    "params.p = DIRECT_MODULUS\n",
    "loader = DataLoader(direct_train_data, batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "checkpoints = direct_results[seed]['checkpoints']\n",
    "hp = get_llc_hyperparams(DIRECT_MODULUS)\n",
    "\n",
    "print(f\"Total checkpoints: {len(checkpoints)}\")\n",
    "print(f\"Hyperparams: lr={hp['lr']:.4f}, Î³={hp['gamma']:.2f}, chains={hp['num_chains']}\")\n",
    "\n",
    "direct_llcs_seed0 = []\n",
    "for i, model_checkpoint in enumerate(checkpoints):\n",
    "    if (i + 1) % 20 == 0 or i == 0 or i == len(checkpoints) - 1:\n",
    "        print(f\"  Checkpoint {i+1}/{len(checkpoints)}\")\n",
    "    \n",
    "    llc_stats = estimate_learning_coeff_with_summary(\n",
    "        model_checkpoint,\n",
    "        loader=loader,\n",
    "        evaluate=evaluate_ce,\n",
    "        sampling_method=SGLD,\n",
    "        optimizer_kwargs=dict(lr=hp['lr'], nbeta=hp['nbeta'], localization=hp['gamma']),\n",
    "        num_chains=hp['num_chains'],\n",
    "        num_draws=hp['num_draws'],\n",
    "        device=DEVICE,\n",
    "        online=False,\n",
    "    )\n",
    "    direct_llcs_seed0.append(llc_stats)\n",
    "\n",
    "llc_values = [llc['llc/mean'] for llc in direct_llcs_seed0]\n",
    "print(f\"\\nCompleted! LLC range: [{min(llc_values):.2f}, {max(llc_values):.2f}]\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05778e73",
   "metadata": {},
   "source": [
    "## 11. Plot LLC Trajectories with Confidence Intervals\n",
    "\n",
    "Visualize LLC evolution across training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b92e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot curriculum LLC vs accuracy for final stage\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Accuracy from all seeds\n",
    "x = np.arange(len(curriculum_final_metrics['val_acc_mean']))\n",
    "ax1.plot(x, curriculum_final_metrics['val_acc_mean'], label='Test Acc (mean)', color='blue')\n",
    "ax1.fill_between(x,\n",
    "                 curriculum_final_metrics['val_acc_mean'] - curriculum_final_metrics['val_acc_std'],\n",
    "                 curriculum_final_metrics['val_acc_mean'] + curriculum_final_metrics['val_acc_std'],\n",
    "                 alpha=0.3, color='blue')\n",
    "\n",
    "# LLC from seed 0\n",
    "llc_values = [llc['llc/mean'] for llc in curriculum_llcs_seed0[CURRICULUM_NUMS[-1]]]\n",
    "ax2.plot(llc_values, color='green', label='LLC (seed 0)', linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Checkpoint')\n",
    "ax1.set_ylabel('Test Accuracy', color='blue')\n",
    "ax2.set_ylabel('LLC (Î»Ì‚)', color='green')\n",
    "ax1.set_title(f'Curriculum (mod-{CURRICULUM_NUMS[-1]}): LLC vs Accuracy')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "save_and_show(fig, 'curriculum_llc_vs_accuracy.png')\n",
    "\n",
    "# Plot direct LLC vs accuracy\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "x = np.arange(len(direct_metrics['val_acc_mean']))\n",
    "ax1.plot(x, direct_metrics['val_acc_mean'], label='Test Acc (mean)', color='blue')\n",
    "ax1.fill_between(x,\n",
    "                 direct_metrics['val_acc_mean'] - direct_metrics['val_acc_std'],\n",
    "                 direct_metrics['val_acc_mean'] + direct_metrics['val_acc_std'],\n",
    "                 alpha=0.3, color='blue')\n",
    "\n",
    "llc_values = [llc['llc/mean'] for llc in direct_llcs_seed0]\n",
    "ax2.plot(llc_values, color='green', label='LLC (seed 0)', linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Checkpoint')\n",
    "ax1.set_ylabel('Test Accuracy', color='blue')\n",
    "ax2.set_ylabel('LLC (Î»Ì‚)', color='green')\n",
    "ax1.set_title(f'Direct (mod-{DIRECT_MODULUS}): LLC vs Accuracy')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "save_and_show(fig, 'direct_llc_vs_accuracy.png')\n",
    "\n",
    "# Comparison plot\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "curriculum_llc = [llc['llc/mean'] for llc in curriculum_llcs_seed0[CURRICULUM_NUMS[-1]]]\n",
    "direct_llc = [llc['llc/mean'] for llc in direct_llcs_seed0]\n",
    "\n",
    "plt.plot(curriculum_llc, label=f'Curriculum (mod-{CURRICULUM_NUMS[-1]})', linewidth=2, color='blue')\n",
    "plt.plot(direct_llc, label=f'Direct (mod-{DIRECT_MODULUS})', linewidth=2, color='orange', linestyle='--')\n",
    "\n",
    "plt.xlabel('Checkpoint', fontsize=12)\n",
    "plt.ylabel('LLC (Î»Ì‚)', fontsize=12)\n",
    "plt.title('LLC Trajectories: Curriculum vs Direct (seed 0, equalized budgets)', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "save_and_show(fig, 'llc_comparison_curriculum_vs_direct.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical_testing",
   "metadata": {},
   "source": [
    "## 12. Statistical Significance Testing\n",
    "\n",
    "Test if curriculum vs direct differences are statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare final test accuracy across seeds\n",
    "curriculum_final_accs = []\n",
    "direct_final_accs = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    curr_df = curriculum_results[seed][CURRICULUM_NUMS[-1]]['df']\n",
    "    curriculum_final_accs.append(curr_df['val_acc'].iloc[-1])\n",
    "    \n",
    "    direct_df = direct_results[seed]['df']\n",
    "    direct_final_accs.append(direct_df['val_acc'].iloc[-1])\n",
    "\n",
    "# T-test\n",
    "t_stat, p_value = stats.ttest_ind(curriculum_final_accs, direct_final_accs)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Test Accuracy Comparison (n={len(SEEDS)} seeds):\")\n",
    "print(f\"  Curriculum: {np.mean(curriculum_final_accs):.4f} Â± {np.std(curriculum_final_accs):.4f}\")\n",
    "print(f\"  Direct:     {np.mean(direct_final_accs):.4f} Â± {np.std(direct_final_accs):.4f}\")\n",
    "print(f\"\\nTwo-sample t-test:\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value:     {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\nâœ… Difference is statistically significant (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Difference is NOT statistically significant (p >= 0.05)\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "pooled_std = np.sqrt((np.var(curriculum_final_accs) + np.var(direct_final_accs)) / 2)\n",
    "cohens_d = (np.mean(curriculum_final_accs) - np.mean(direct_final_accs)) / pooled_std\n",
    "print(f\"  Cohen's d:   {cohens_d:.4f}\")\n",
    "\n",
    "if abs(cohens_d) < 0.2:\n",
    "    effect = \"negligible\"\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    effect = \"small\"\n",
    "elif abs(cohens_d) < 0.8:\n",
    "    effect = \"medium\"\n",
    "else:\n",
    "    effect = \"large\"\n",
    "print(f\"  Effect size: {effect}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "positions = [1, 2]\n",
    "data = [curriculum_final_accs, direct_final_accs]\n",
    "labels = ['Curriculum', 'Direct']\n",
    "\n",
    "bp = ax.boxplot(data, positions=positions, labels=labels, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['blue', 'orange']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "# Add individual points\n",
    "for i, (pos, vals) in enumerate(zip(positions, data)):\n",
    "    x = np.random.normal(pos, 0.04, len(vals))\n",
    "    ax.scatter(x, vals, alpha=0.6, s=50, color=['blue', 'orange'][i])\n",
    "\n",
    "ax.set_ylabel('Final Test Accuracy')\n",
    "ax.set_title(f'Final Test Accuracy Distribution (n={len(SEEDS)} seeds)\\np-value = {p_value:.4f}')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "save_and_show(fig, 'final_accuracy_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 13. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ”§ Experimental Setup:\")\n",
    "print(f\"  â€¢ Curriculum stages: {CURRICULUM_NUMS}\")\n",
    "print(f\"  â€¢ Total batches: {CURRICULUM_TOTAL_BATCHES:,} (both curriculum and direct)\")\n",
    "print(f\"  â€¢ Seeds: {len(SEEDS)}\")\n",
    "print(f\"  â€¢ LLC chains: 3\")\n",
    "print(f\"  â€¢ Weight transfer: Pad/crop (not random reinitialization)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Final Results (mean Â± std across seeds):\")\n",
    "print(f\"  Curriculum final test acc: {np.mean(curriculum_final_accs):.4f} Â± {np.std(curriculum_final_accs):.4f}\")\n",
    "print(f\"  Direct final test acc:     {np.mean(direct_final_accs):.4f} Â± {np.std(direct_final_accs):.4f}\")\n",
    "print(f\"  Difference:                {np.mean(curriculum_final_accs) - np.mean(direct_final_accs):.4f}\")\n",
    "print(f\"  Statistical significance:  p = {p_value:.4f} ({'sig' if p_value < 0.05 else 'not sig'})\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ LLC Statistics (seed 0):\")\n",
    "for stage_idx, p in enumerate(CURRICULUM_NUMS):\n",
    "    llc_vals = [llc['llc/mean'] for llc in curriculum_llcs_seed0[p]]\n",
    "    print(f\"  Curriculum stage {stage_idx+1} (mod-{p}):\")\n",
    "    print(f\"    Initial LLC: {llc_vals[0]:.2f}\")\n",
    "    print(f\"    Final LLC:   {llc_vals[-1]:.2f}\")\n",
    "\n",
    "direct_llc_vals = [llc['llc/mean'] for llc in direct_llcs_seed0]\n",
    "print(f\"\\n  Direct (mod-{DIRECT_MODULUS}):\")\n",
    "print(f\"    Initial LLC: {direct_llc_vals[0]:.2f}\")\n",
    "print(f\"    Final LLC:   {direct_llc_vals[-1]:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
